{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised sentiment analysis on reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JDtTZDcNbdvd"
   },
   "outputs": [],
   "source": [
    "#!pip install gensim\n",
    "#!python -m spacy download pt_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JHI-yO0cbPtk"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re  # For preprocessing\n",
    "from time import time  # To time our operations\n",
    "from collections import defaultdict  # For word frequency\n",
    "import spacy  # For preprocessing\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "import gensim\n",
    "import spacy\n",
    "import pt_core_news_sm\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kdACiC9WbIk_"
   },
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OYsNSBOpa2Jd"
   },
   "source": [
    "__Importer les données__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b6Ra3bH3a2Je"
   },
   "outputs": [],
   "source": [
    "order_reviews = pd.read_csv(\"Data/olist_order_reviews_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k0qOzzGma2Ji"
   },
   "outputs": [],
   "source": [
    "comments = order_reviews[[\"review_comment_message\",\"review_score\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lbumJ6ZBa2Jl"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(comments.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CZc0FS_ta2Jt"
   },
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5ETGHJkoa2Jw",
    "outputId": "d20faafd-822a-4c5e-9f77-bde86cf0e8a6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41753, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LEyymOBna2J6"
   },
   "outputs": [],
   "source": [
    "nlp = pt_core_news_sm.load(disable=['ner','parser'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1RRzh5Vba2J9",
    "outputId": "0dfb6ed9-019a-452c-d3c0-8c7c84e1d357"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.pt.Portuguese at 0x1f296f14f48>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y5peliW2a2KB"
   },
   "outputs": [],
   "source": [
    "#modification manuelle de la liste des stop words\n",
    "nlp.vocab[\"bem\"].is_stop = False\n",
    "nlp.vocab[\"boa\"].is_stop = False\n",
    "nlp.vocab[\"muito\"].is_stop = False\n",
    "nlp.vocab[\"muitos\"].is_stop = False\n",
    "nlp.vocab[\"menos\"].is_stop = False\n",
    "nlp.vocab[\"mal\"].is_stop = False\n",
    "nlp.vocab[\"bom\"].is_stop = False\n",
    "nlp.vocab[\"boa\"].is_stop = False\n",
    "nlp.vocab[\"sem\"].is_stop = False\n",
    "nlp.vocab[\"bem\"].is_stop = False\n",
    "nlp.vocab[\"obrigado\"].is_stop = False\n",
    "nlp.vocab[\"obrigada\"].is_stop = False\n",
    "nlp.vocab[\"o\"].is_stop = True\n",
    "nlp.vocab[\"e\"].is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iYVGsg9qa2KF"
   },
   "outputs": [],
   "source": [
    "def cleaning(doc):\n",
    "    # Lemmatizes and removes stopwords\n",
    "    # doc needs to be a spacy Doc object\n",
    "    txt = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "    # Word2Vec uses context words to learn the vector representation of a target word,\n",
    "    # if a sentence is only one or two words long,\n",
    "    # the benefit for the training is very small\n",
    "    if len(txt) > 2:\n",
    "        return ' '.join(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KD_0PkFja2KJ"
   },
   "outputs": [],
   "source": [
    "#retrait des elements de moins de 3 caractères\n",
    "brief_cleaning = (re.sub(\"[^A-Za-z']{3,}\", ' ', str(row)).lower() for row in df['review_comment_message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "puxFMgmQa2KM",
    "outputId": "e1461c75-ba96-42a0-c2c7-2d00dabab12f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to clean up everything: 1.81 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "txt = [cleaning(doc) for doc in nlp.pipe(brief_cleaning, batch_size=5000, n_threads=-1)]\n",
    "\n",
    "print('Time to clean up everything: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dqg-NGwna2KQ",
    "outputId": "ab78c37b-425d-4d0f-bcc9-5bbd6e03e0ed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31161, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = pd.DataFrame({'clean': txt, 'rate':df.review_score})\n",
    "df_clean = df_clean.dropna().drop_duplicates()\n",
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yvehxIAta2KX"
   },
   "outputs": [],
   "source": [
    "sent = [row.split() for row in df_clean['clean']] # divise chaque commentaire en mots = liste de liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sNjq89tsa2Kc",
    "outputId": "60626a98-59b7-4420-ca08-b346ddcc6764"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['receber', 'bem', 'prazo', 'estipular']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L-fxUNeKa2Kj"
   },
   "outputs": [],
   "source": [
    "phrases = Phrases(sent, min_count=30, progress_per=10000) #Creates the relevant phrases from the list of sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nfVdf9UAa2Kp"
   },
   "outputs": [],
   "source": [
    "bigram = Phraser(phrases) \n",
    "#he goal of Phraser() is to cut down memory consumption of Phrases(), by discarding model state not strictly needed for the bigram detection task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gZq7wmR_a2Kt"
   },
   "outputs": [],
   "source": [
    "sentences = bigram[sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KowCMAB_a2Kx",
    "outputId": "cf0f21f6-c22a-42e4-e21e-be189808b4bb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31161"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DDUPxeU5a2K1",
    "outputId": "43ceabbe-cdd6-4096-a97c-8837a5c88b77"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['receber', 'bem', 'prazo_estipular']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ysBsC2eaa2K5",
    "outputId": "25b5f7ff-24dc-430b-dd94-b7f847167923"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11487"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq = defaultdict(int)\n",
    "for sent in sentences:\n",
    "    for i in sent:\n",
    "        word_freq[i] += 1\n",
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1m1gijmca2K_",
    "outputId": "d2615afd-2f7c-4795-f989-1480e5a8d959"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['produto',\n",
       " 'o',\n",
       " 'entregar',\n",
       " 'comprar',\n",
       " 'prazo',\n",
       " 'muito',\n",
       " 'chegar',\n",
       " 'receber',\n",
       " 'bom',\n",
       " 'vir',\n",
       " 'recomendar',\n",
       " 'loja',\n",
       " 'pedir',\n",
       " 'bem',\n",
       " 'dia',\n",
       " 'gostar',\n",
       " 'qualidade',\n",
       " 'rápido',\n",
       " 'esperar',\n",
       " 'excelente']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mots les plus frequent\n",
    "sorted(word_freq, key=word_freq.get, reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bGY96k45a2LC"
   },
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count() # Count the number of cores in a computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ulw1pF8Ia2LJ"
   },
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(min_count=100, # Ignores all words with total absolute frequency lower than this - (2, 100)\n",
    "                     window=2,\n",
    "                     size=300,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=cores-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0V3PL5qRa2LM",
    "outputId": "3092fbe0-980a-4869-83ca-86bb5dcb1386"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.03 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "w2v_model.build_vocab(sentences, progress_per=10000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S4rPsBv3a2LP",
    "outputId": "8876fe33-3e34-4da1-c6b4-b4061cecc863"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "325"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w2v_model.wv.vocab.keys()) # Nombre de mots dans le vocabulaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3A93Cu-Ba2LS",
    "outputId": "c992623b-6a3e-449c-d63b-6d1534516923"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['receber', 'bem', 'prazo_estipular']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XL8nLdYKa2LV",
    "outputId": "8986f36c-daf3-4eb5-fd50-58ee8d756f1e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['receber', 'bem', 'prazo_estipular', 'parabém', 'loja', 'lannister', 'adorar', 'comprar', 'internet', 'o', 'feliz', 'aparelhar', 'eficiente', 'site', 'marcar', 'chegar', 'outro', 'corretar', 'bom', 'vendedor', 'confiável', 'produto', 'ok', 'entregar', 'prazo', 'gostar', 'haver', 'obrigar', 'realizar', 'muito', 'dar', 'usar', 'presentar', 'sem', 'problema', 'relógio', 'bonito', 'ocorrer', 'combinar', 'acreditar', 'stark', 'exatamente', 'esperar', 'encomendar', 'atrasar', 'super', 'otimo', 'atar', 'processar', 'tomar', 'q', 'controlo', 'faltar', 'ser', 'satisfeito', 'amar', 'achar', 'lindar', 'ótima', 'bem_embalar', 'qualidade', 'fretar', 'pq', 'casar', 'solicitar', 'unidade', 'vir', 'mim', 'foto', 'anúncio', 'inferior', 'mal', 'acabar', 'kit', 'mochila', 'pedir', 'reembolsar', 'respostar', 'parabens', 'conseguir', 'prometer', 'montar', 'vender', 'targaryen', 'tapetar', 'rápido', 'tecer', 'so', 'hoje', 'certar', 'bolsar', 'lindo', 'acordar', 'anunciar', 'super_rápido', 'demorar', 'pra', 'rapido', 'datar', 'pensar', 'enviar', 'recomendar', 'cliente', 'jogar', 'timo', 'noto', 'constar', 'pagar', 'caro', 'melhor', 'excelente', 'atenção', 'ter', 'quebrar', 'caixa', 'amassar', 'vcs', 'servir', 'dia', 'residência', 'correio', 'd', 'mau', 'fazer', 'melhorar', 'semana', 'baratheon', 'ficar', 'pendente', 'parceiro', 'abrir', 'reclamação', 'alto', 'descrever', 'rastrear', 'dever', 'precisar', 'testar', 'capar', 'rapida', 'quase', 'horar', 'ver', 'e-mail', 'aguardar', 'acontecer', 'nenhum', 'retornar', 'insatisfeito', 'voltar', 'mercadoria', 'fácil', 'nao', 'manual', 'encaixar', 'diferente', 'atendimento', 'mesmo', 'preto', 'defeito', 'ja', 'apesar', 'reclamar', 'estar', 'toalha', 'rastreamento', 'levar', 'modelar', 'cor', 'satisfazer', 'cancelar', 'ligar', 'cobrar', 'trocar', 'endereçar', 'ir', 'certo', 'passar', 'total', 'devolver', 'perfazer', 'maravilhoso', 'tentar', 'noto_fiscal', 'avisar', 'correar', 'cancelamento', 'satisfação', 'deixar', 'desejar', 'material', 'acabamento', 'corretamente', 'verificar', 'perfeito', 'embalagem', 'danificar', 's', 'necessidade', 'cabo', 'ótimo', 'embalar', 'funcionar', 'mandar', 'direito', 'mau_qualidade', 'novamente', 'empresar', 'sair', 'absurdo', 'informar', 'conformar_anunciar', 'trabalhar', 'indicar', 'atender', 'ótima_qualidade', 'colocar', 'instalar', 'cartucho', 'apresentar', 'errar', 'cumprir', 'fraco', 'devolução', 'dinheiro', 'lençol', 'algum', 'email', 'prever', 'n', 'resistente', 'conformar', 'preço', 'pôr', 'igual', 'celular', 'tirar', 'tamanho', 'descrição', 'ano', 'datar_previsto', 'resolver', 'contato', 'valer', 'responder', 'longo', 'cadeira', 'poder', 'expectativa', 'estabelecer', 'mostrar', 'dizer', 'ater', 'plástico', 'único', 'falar', 'pessoa', 'feito', 'cartão', 'pequeno', 'medir', 'aguardar_retornar', 'resolver_problema', 'ninguém', 'superar_expectativa', 'previsão', 'cortinar', 'peno', 'suportar', 'ruim', 'mto', 'infelizmente', 'agradecer', 'dinheiro_voltar', 'branco', 'perfeito_condição', 'imaginar', 'telefonar', 'pagar_fretar', 'totalmente', 'decepcionar', 'informação', 'retirar', 'fornecedor', 'continuar', 'consumidor', 'simples', 'querer', 'item', 'imagem', 'respeitar', 'corresponder', 'película', 'escolher', 'estornar', 'entrar', 'serviço', 'adquirir', 'condição', 'original', 'atender_expectativa', 'pagamento', 'avariar', 'entrar_contato', 'rapidez', 'avaliar', 'juntar', 'p', 'jeito', 'frágil', 'menos', 'contar', 'realmente', 'parecer', 'lacrar', 'carro', 'entregue', 'motivar', 'buscar', 'lannister.com', 'estocar', 'filho', 'tima', 'transportador', 'status'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zINsvbx-a2LY",
    "outputId": "e18e128b-94eb-45e2-f6ab-5cd4a7818bd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 0.86 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TH24qi5fa2Lb"
   },
   "outputs": [],
   "source": [
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TBzPtruWa2Ld",
    "outputId": "b77ad9d9-72a7-47c3-d426-bccc58b33781"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(325, 300)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cuJzrXQ4a2Li",
    "outputId": "05820485-c9cc-4976-a59a-ec09c99a7942"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('quebrar', 0.9922207593917847),\n",
       " ('cabo', 0.9915614128112793),\n",
       " ('danificar', 0.9891204833984375),\n",
       " ('faltar', 0.9863587617874146),\n",
       " ('carro', 0.9827027320861816),\n",
       " ('funcionar', 0.9801025390625),\n",
       " ('lençol', 0.976325273513794),\n",
       " ('modelar', 0.9739847183227539),\n",
       " ('cadeira', 0.9726841449737549),\n",
       " ('controlo', 0.9692246317863464)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"defeito\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MkLe-iHecMhI"
   },
   "source": [
    "# Il faut choisir parmit les deux kmeans, NE PAS LANCER LES DEUX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0vtTGwNiHIDM"
   },
   "source": [
    "## 1 - K-means from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KWrPsRHo2bTq"
   },
   "outputs": [],
   "source": [
    "zero = np.zeros((int(325/2),1))\n",
    "one = np.ones((int(325/2)+1,1))\n",
    "lab = np.concatenate((zero,one), axis=0)\n",
    "data = np.concatenate((w2v_model.wv.vectors, lab), axis=1)\n",
    "data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mTBGISdX1Y3Q"
   },
   "outputs": [],
   "source": [
    "def centroid(data):\n",
    "    var = data.columns\n",
    "    m = len(var)\n",
    "    l = []\n",
    "    for i in range(m):\n",
    "        l.append(var[i])\n",
    "        l[i] = np.mean(data[var[i]])\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "men52CtV1YdP"
   },
   "outputs": [],
   "source": [
    "def graphcentre(data,nb_class,nbvar):\n",
    "    v = locals()\n",
    "    cen = []\n",
    "    for i in range(nb_class):\n",
    "        d = data.where(data.iloc[:,-1]==i)\n",
    "        v['c%d' % i] = centroid(d.iloc[:,:nbvar])\n",
    "        cen.append(v['c%d' % i])\n",
    "    return cen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iTVDHe071YMT"
   },
   "outputs": [],
   "source": [
    "def dist(data,centre):\n",
    "    dist=0\n",
    "    for i in range(len(data)):\n",
    "        dist = dist + (centre[i]-data[i])**2\n",
    "    dist = np.sqrt(dist)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CoCAZp5a1X51"
   },
   "outputs": [],
   "source": [
    "def kmean(data,nbvar,nb_class):\n",
    "    l = 0\n",
    "    m = 1\n",
    "    count = 0\n",
    "    while l != m:\n",
    "        if count > 0:\n",
    "            l = [sum(centre[i]) for i in range(nb_class)]\n",
    "        centre = graphcentre(data,nb_class,nbvar)\n",
    "        m = [sum(centre[i]) for i in range(nb_class)]\n",
    "        di = []\n",
    "        v = locals()\n",
    "        for j in range(nb_class):\n",
    "            v['dist%d' % j] = np.zeros((len(data),1))\n",
    "            for i in range(len(data)):\n",
    "                v['dist%d' % j][i] = dist(data.iloc[i,:nbvar],centre[j])\n",
    "            di.append(v['dist%d' % j])\n",
    "    \n",
    "        label = [] \n",
    "        distance = []   \n",
    "        for i in range(len(data)):\n",
    "            sel = []\n",
    "            for j in range(nb_class):\n",
    "                element = di[j]\n",
    "                sel.append(element[i])\n",
    "            label.append(sel.index(np.min(sel)))\n",
    "            distance.append(min(sel))\n",
    "\n",
    "        data['predict_label'] = label\n",
    "        count += 1\n",
    "    data['distance'] = distance\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "4E0rS-pn1W2c",
    "outputId": "5f5c8618-03c4-4379-d818-914286104e75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "kmean(data,50,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0G8ClgCK60pS"
   },
   "outputs": [],
   "source": [
    "data['cluster_value'] = [-1 if i==0 else 1 for i in data.predict_label]\n",
    "data['sentiment_coeff'] = data.distance * data.cluster_value\n",
    "data['distance'] = data.distance.apply(lambda x: x[0])\n",
    "data['sentiment_coeff'] = data.sentiment_coeff.apply(lambda x: x[0])\n",
    "data['words'] = w2v_model.wv.vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "047EFzoL81-t"
   },
   "outputs": [],
   "source": [
    "sentiment_map = data[['words', 'sentiment_coeff']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "caimtCZE-u4h"
   },
   "outputs": [],
   "source": [
    "sentiment_dict = dict(zip(sentiment_map.words.values, sentiment_map.sentiment_coeff.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jAGGHXIN67hK"
   },
   "source": [
    "## 2 - K-means sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-QaRAqeT08db"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ctu7IwXX08df"
   },
   "outputs": [],
   "source": [
    "word_vectors = w2v_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "3foJKm_6Zfeh",
    "outputId": "57640144-8706-440b-fca2-06428da64426"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(186, 50)"
      ]
     },
     "execution_count": 481,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e_gOcqBj08dj"
   },
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=2, max_iter=1000, random_state=True, n_init=50).fit(X=word_vectors.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ToUQKWeA08dm"
   },
   "outputs": [],
   "source": [
    "positive_cluster_center = model.cluster_centers_[0]\n",
    "negative_cluster_center = model.cluster_centers_[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "X4qGlEp308ds",
    "outputId": "ae9956c4-219c-48be-f286-a99a3aacbf35"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('certo', 0.9963061809539795),\n",
       " ('expectativa', 0.996258556842804),\n",
       " ('melhor', 0.9929500818252563),\n",
       " ('acordar', 0.9922909736633301),\n",
       " ('prazo_estipular', 0.991629958152771),\n",
       " ('exatamente', 0.9910224676132202),\n",
       " ('certar', 0.9904945492744446),\n",
       " ('combinar', 0.9900484085083008),\n",
       " ('testar', 0.9883270263671875),\n",
       " ('ok', 0.9883203506469727)]"
      ]
     },
     "execution_count": 484,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.similar_by_vector(model.cluster_centers_[1], topn=10,  restrict_vocab=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "HOGaLUCY08dz",
    "outputId": "8a984703-a0ce-4d35-ced3-16aa67a8c575"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186"
      ]
     },
     "execution_count": 485,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.vectors.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "colab_type": "code",
    "id": "dxgIEGW108d2",
    "outputId": "0d52428d-7b68-4876-c9fd-8d29ebdc589e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "words = pd.DataFrame(word_vectors.wv.vocab.keys())\n",
    "words.columns = ['words']\n",
    "words['vectors'] = words.words.apply(lambda x: word_vectors.wv[f'{x}'])\n",
    "words['cluster'] = words.vectors.apply(lambda x: model.predict([np.array(x)]))\n",
    "words.cluster = words.cluster.apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LY1Tivzc08d7"
   },
   "outputs": [],
   "source": [
    "words['cluster_value'] = [-1 if i==0 else 1 for i in words.cluster]\n",
    "words['closeness_score'] = words.apply(lambda x: 1/(model.transform([x.vectors]).min()), axis=1)\n",
    "words['sentiment_coeff'] = words.closeness_score * words.cluster_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lIe8UbnI08d_"
   },
   "outputs": [],
   "source": [
    "words[['words', 'sentiment_coeff']].to_csv('sentiment_dictionary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Cid15cZ08eB"
   },
   "outputs": [],
   "source": [
    "sentiment_map = words[['words', 'sentiment_coeff']]\n",
    "sentiment_dict = dict(zip(sentiment_map.words.values, sentiment_map.sentiment_coeff.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4jFbWwjQbeVZ"
   },
   "source": [
    "# tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PBBHbpfX08eD"
   },
   "outputs": [],
   "source": [
    "file_weighting = df_clean.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KsjPZuIM08eG"
   },
   "outputs": [],
   "source": [
    "file_weighting = file_weighting.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "id": "S_SxBQOf08eK",
    "outputId": "72072f72-09f7-44ec-846b-95dfbe7b798f"
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=lambda y: y.split(), norm=None)\n",
    "tfidf.fit(file_weighting.clean)\n",
    "features = pd.Series(tfidf.get_feature_names())\n",
    "transformed = tfidf.transform(file_weighting.clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G1iE6Wuc08eS"
   },
   "outputs": [],
   "source": [
    "def create_tfidf_dictionary(x, transformed_file, features):\n",
    "    '''\n",
    "    create dictionary for each input sentence x, where each word has assigned its tfidf score\n",
    "    \n",
    "    inspired  by function from this wonderful article: \n",
    "    https://medium.com/analytics-vidhya/automated-keyword-extraction-from-articles-using-nlp-bfd864f41b34\n",
    "    \n",
    "    x - row of dataframe, containing sentences, and their indexes,\n",
    "    transformed_file - all sentences transformed with TfidfVectorizer\n",
    "    features - names of all words in corpus used in TfidfVectorizer\n",
    "\n",
    "    '''\n",
    "    vector_coo = transformed_file[x.name].tocoo()\n",
    "    vector_coo.col = features.iloc[vector_coo.col].values\n",
    "    dict_from_coo = dict(zip(vector_coo.col, vector_coo.data))\n",
    "    return dict_from_coo\n",
    "\n",
    "def replace_tfidf_words(x, transformed_file, features):\n",
    "    '''\n",
    "    replacing each word with it's calculated tfidf dictionary with scores of each word\n",
    "    x - row of dataframe, containing sentences, and their indexes,\n",
    "    transformed_file - all sentences transformed with TfidfVectorizer\n",
    "    features - names of all words in corpus used in TfidfVectorizer\n",
    "    '''\n",
    "    dictionary = create_tfidf_dictionary(x, transformed_file, features)  \n",
    "    return list(map(lambda y:dictionary[f'{y}'], x.clean.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "eQY3rh9K08eU",
    "outputId": "37a2642d-7854-4b4c-a998-d9c66c433d00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 19.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "replaced_tfidf_scores = file_weighting.apply(lambda x: replace_tfidf_words(x, transformed, features), axis=1)\n",
    "#this step takes around 3-4 minutes minutes to calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kp5XvAVB08fx"
   },
   "outputs": [],
   "source": [
    "def replace_sentiment_words(word, sentiment_dict):\n",
    "    '''\n",
    "    replacing each word with its associated sentiment score from sentiment dict\n",
    "    '''\n",
    "    try:\n",
    "        out = sentiment_dict[word]\n",
    "    except KeyError:\n",
    "        out = 0\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A0hG9mt308f1"
   },
   "outputs": [],
   "source": [
    "replaced_closeness_scores = file_weighting.clean.apply(lambda x: list(map(lambda y: replace_sentiment_words(y, sentiment_dict), x.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nURgt2LskI_2"
   },
   "outputs": [],
   "source": [
    "replacement_df = pd.DataFrame(data=[replaced_closeness_scores, replaced_tfidf_scores, file_weighting.clean, file_weighting.rate]).T\n",
    "replacement_df.columns = ['sentiment_coeff', 'tfidf_scores', 'sentence', 'Rate']\n",
    "replacement_df['sentiment_rate'] = replacement_df.apply(lambda x: np.array(x.loc['sentiment_coeff']) @ np.array(x.loc['tfidf_scores']), axis=1)\n",
    "replacement_df['prediction'] = (replacement_df.sentiment_rate>0).astype('int8')\n",
    "replacement_df['sentiment'] = [1 if i>=2.5 else 0 for i in replacement_df.Rate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 332
    },
    "colab_type": "code",
    "id": "0xD3-sCDk4MM",
    "outputId": "a3cf84a6-f888-4bc8-986e-8465ff1dd683"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2348</td>\n",
       "      <td>7593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>6469</td>\n",
       "      <td>14751</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0      1\n",
       "0  2348   7593\n",
       "1  6469  14751"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      " Scores\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>accuracy</td>\n",
       "      <td>0.548731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>precision</td>\n",
       "      <td>0.660177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>recall</td>\n",
       "      <td>0.695146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>f1</td>\n",
       "      <td>0.677211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             scores\n",
       "accuracy   0.548731\n",
       "precision  0.660177\n",
       "recall     0.695146\n",
       "f1         0.677211"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predicted_classes = replacement_df.prediction\n",
    "y_test = replacement_df.sentiment\n",
    "\n",
    "conf_matrix = pd.DataFrame(confusion_matrix(replacement_df.sentiment, replacement_df.prediction))\n",
    "print('Confusion Matrix')\n",
    "display(conf_matrix)\n",
    "\n",
    "test_scores = accuracy_score(y_test,predicted_classes), precision_score(y_test, predicted_classes), recall_score(y_test, predicted_classes), f1_score(y_test, predicted_classes)\n",
    "\n",
    "print('\\n \\n Scores')\n",
    "scores = pd.DataFrame(data=[test_scores])\n",
    "scores.columns = ['accuracy', 'precision', 'recall', 'f1']\n",
    "scores = scores.T\n",
    "scores.columns = ['scores']\n",
    "display(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "NLP-Kmeans.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
